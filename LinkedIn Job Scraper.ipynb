{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guides Used:\n",
    "Initial tutorial: https://maoviola.medium.com/a-complete-guide-to-web-scraping-linkedin-job-postings-ad290fcaa97f\n",
    "\n",
    "For login: https://www.geeksforgeeks.org/scrape-linkedin-using-selenium-and-beautiful-soup-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper runs first by starting a webdriver instance. Then, it'll navigate to the LinkedIn login page and log in. The rest of the scrape is set up in 2 function below. Empty lists are intialized globally so they can be accessed by all cells of the notebook. The scraper loop function takes in a LinkedIn job search url, a starting page number, and an ending page number. It loads the starting url, navigates to the bottom of the page, and presses the button corresponding to the starting page number. Then, it calls the scrape function which grabs a list of all jobs on the page and navigates through them one by one. It clicks on the job card to pull up the more detailed window with the job description, and scrapes the information from it. Then, it naviagtes to the next job in the list and repeats the process. Once it has scraped all jobs in the list, it returns to the scraper loop function which navigates to the next page and calls the scrape function again. Once the loop detects it is on the same page as then given ending page number, it'll call the scraper one last time and end the loop. A list of the current datetime is then concatenated with the rest of the job lists into a pandas dataframe where it is exported into a csv file. The csv files are then re-imported into the project and concatenated into one giant pandas dataframe. This dataframe is given basic formatting and verified that all the cells look correct. Then, it's given advanced formatting where select columns are split using regex into multiple columns and other select columns are formatted with True/ False values. Then, the description is analyzed using spacy into keywords, namely key skills, which are then one-hot encoded. Each listing is then manually rating from 1 to 3 based on how relevant and promising a listing it is. The whole dataframe is then split into training and test sets. The training set is used to train a logistic regression model, verified with the test set. New data is then obtained through the scraper and passed through the cleaning and transformation pipeline before being sent through the regression model and categorized by relevance. A final function returns most recent and highest-rated listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_url = \"https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin\"\n",
    "print(\"logging in\")\n",
    "wd.get(login_url)\n",
    "time.sleep(2)\n",
    "user = wd.find_element(By.ID, \"username\")\n",
    "user.send_keys(\"****YOUR USERNAME HERE****\")\n",
    "passw = wd.find_element(By.ID, \"password\")\n",
    "passw.send_keys(\"****YOUR PASSWORD HERE****\")\n",
    "wd.find_element(By.XPATH, \"//button[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the jobs\n",
    "Only 25 load per page so we need to scroll to the bottom of the page to load all 25, scrape them, and move to the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize lists globally so all funcs can access\n",
    "ids = []\n",
    "date_posted = []\n",
    "date_scraped = []\n",
    "title = [] \n",
    "company = []\n",
    "location = []\n",
    "promoted_easyapply = []\n",
    "emp_info = []\n",
    "co_info = []\n",
    "connections_alumni = []\n",
    "link = []\n",
    "descr = []\n",
    "poster_name = []\n",
    "poster_link = []\n",
    "job_title_category = []\n",
    "\n",
    "def initialize():\n",
    "    #used to re-initialize lists in case of testing, errors, or new scrapes\n",
    "    answ = input(\"Are you sure you want to empty all lists? Y/N\")\n",
    "    if answ.lower() in [\"n\", \"no\"]:\n",
    "        print(\"NOT reinitializing lists\")\n",
    "        return\n",
    "    \n",
    "    print(\"reinitializing lists\")\n",
    "    global ids, date_posted, date_scraped, title, company, location, promoted_easyapply, emp_info, co_info, connections_alumni, link, descr, poster_name, poster_link, job_title_category\n",
    "    ids = []\n",
    "    date_posted = []\n",
    "    date_scraped = []\n",
    "    title = [] \n",
    "    company = []\n",
    "    location = []\n",
    "    promoted_easyapply = []\n",
    "    emp_info = []\n",
    "    co_info = []\n",
    "    connections_alumni = []\n",
    "    link = []\n",
    "    descr = []\n",
    "    poster_name = []\n",
    "    poster_link = []\n",
    "    job_title_category = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_loop(start_url, start_pg=1, end_pg=5):\n",
    "    print(\"loading starting page\")\n",
    "    wd.get(start_url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    numJobs = wd.find_element(By.CLASS_NAME, \"jobs-search-results-list__subtitle\").get_attribute(\"innerText\")[:-8]\n",
    "    numJobs = int(numJobs.replace(\",\",\"\"))\n",
    "    print(\"found \"+str(numJobs)+\" jobs posted in the last 24 hours\")\n",
    "    \n",
    "    max_pgs = math.ceil(numJobs/25)\n",
    "    end_pg = min(max_pgs, end_pg)\n",
    "    \n",
    "    total_scrape_time = ((end_pg - start_pg) + 1) * 195\n",
    "    print(\"maximum scrape time: roughly \"+str(total_scrape_time)+\" seconds\")\n",
    "    \n",
    "    curr_pg = start_pg\n",
    "    while curr_pg <= end_pg:\n",
    "        print(\"loading page \"+str(curr_pg)+\" of \"+str(end_pg))\n",
    "        pages = wd.find_element(By.CLASS_NAME, \"artdeco-pagination__pages.artdeco-pagination__pages--number\").find_elements(By.CSS_SELECTOR, \"button\")\n",
    "        #if len(pages) < curr_pg:\n",
    "            #break\n",
    "    \n",
    "        if curr_pg >= 10:\n",
    "            pages[6].click()\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            pages[curr_pg-1].click()\n",
    "            time.sleep(5)\n",
    "            \n",
    "        \n",
    "        print(\"scrolling to bottom of page\")\n",
    "        for i in range(0,5):\n",
    "            wd.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        jobs = wd.find_element(By.CLASS_NAME, \"jobs-search-results-list\").find_elements(By.CLASS_NAME, \"jobs-search-results__list-item\")\n",
    "        print(\"found \"+str(len(jobs))+\" of maximum 25 jobs\")\n",
    "        \n",
    "            \n",
    "        curr_pg+=1\n",
    "        \n",
    "        print(\"scraping...\")\n",
    "        scrape(jobs)\n",
    "        \n",
    "    #done with the loop and the scrape!\n",
    "    print(\"DONE!!\")\n",
    "        \n",
    "    \n",
    "def scrape(jobs):\n",
    "    num_jobs = str(len(jobs))\n",
    "    i = 1\n",
    "    for job in jobs:\n",
    "        print(str(i)+\"/\"+num_jobs)\n",
    "        i+=1\n",
    "        job.click()\n",
    "        time.sleep(2)\n",
    "    \n",
    "        #job id\n",
    "        ids.append(job.find_element(By.CLASS_NAME, \"job-card-container\").get_attribute(\"data-job-id\"))\n",
    "    \n",
    "        #job title\n",
    "        title.append(job.find_element(By.CLASS_NAME, \"job-card-list__title\").get_attribute(\"innerText\"))\n",
    "        #print(title[-1])\n",
    "    \n",
    "        #job company\n",
    "        company.append(job.find_element(By.CLASS_NAME, \"job-card-container__primary-description\").get_attribute(\"innerText\"))\n",
    "    \n",
    "        #job location\n",
    "        location.append(job.find_element(By.CLASS_NAME, \"job-card-container__metadata-item\").get_attribute(\"innerText\"))\n",
    "    \n",
    "        #easy apply/ promoted\n",
    "        promoted_easyapply.append([i.get_attribute(\"innerText\") for i in job.find_elements(By.CLASS_NAME, \"job-card-list__footer-wrapper\")])\n",
    "    \n",
    "        #advanced info holding variable\n",
    "        job.click()\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            adv = wd.find_elements(By.CLASS_NAME, \"jobs-unified-top-card__job-insight\")\n",
    "            icons = [i.find_element(By.CSS_SELECTOR, \"li-icon\").get_attribute(\"type\") for i in adv]\n",
    "    \n",
    "            #job info\n",
    "            if \"job\" in icons:\n",
    "                emp_info.append(adv.pop(0).get_attribute(\"innerText\"))\n",
    "            else:\n",
    "                emp_info.append(np.NaN)\n",
    "    \n",
    "            #company info\n",
    "            if \"company\" in icons:\n",
    "                co_info.append(adv.pop(0).get_attribute(\"innerText\"))\n",
    "            else:\n",
    "                co_info.append(np.NaN)\n",
    "    \n",
    "            #alumni and connections\n",
    "            if \"people\" in icons:\n",
    "                ppl = adv.pop(0)\n",
    "                connections_alumni.append([ppl.get_attribute(\"innerText\"), [i.get_attribute(\"href\") for i in ppl.find_elements(By.CLASS_NAME, \"app-aware-link\")]])\n",
    "            else:\n",
    "                connections_alumni.append(np.NaN)\n",
    "        except:\n",
    "            #if there's a problem getting the icons (sometimes it can't find li-icon) just append np.NaN and manually fill in when cleaning\n",
    "            emp_info.append(np.NaN)\n",
    "            co_info.append(np.NaN)\n",
    "            connections_alumni.append(np.NaN)\n",
    "            \n",
    "    \n",
    "        #job listing link\n",
    "        link.append(job.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\"))\n",
    "    \n",
    "        #job description\n",
    "        descr.append(wd.find_element(By.CLASS_NAME, \"jobs-box__html-content\").get_attribute(\"innerText\"))\n",
    "    \n",
    "        #hirer info\n",
    "        try:\n",
    "            hirer_box = wd.find_element(By.CLASS_NAME, \"hirer-card__container\")\n",
    "            poster_name.append(hirer_box.find_element(By.CLASS_NAME, \"jobs-poster__name\").get_attribute(\"innerText\"))\n",
    "            poster_link.append(hirer_box.find_element(By.CLASS_NAME, \"app-aware-link\").get_attribute(\"href\"))\n",
    "        except:\n",
    "            poster_name.append(np.NaN)\n",
    "            poster_link.append(np.NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data scientist 24-hr scraper\n",
    "url = \"https://www.linkedin.com/jobs/search/?currentJobId=3705632803&f_TPR=r86400&geoId=90000084&keywords=Data%20Scientist&location=San%20Francisco%20Bay%20Area&refresh=true\"\n",
    "new_url = \"https://www.linkedin.com/jobs/search/?currentJobId=3714619029&f_TPR=r86400&geoId=90000084&keywords=Data%20Scientist&location=San%20Francisco%20Bay%20Area&refresh=true&start=900\"\n",
    "scraper_loop(new_url, 37, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data analyst 24-hr scraper\n",
    "url = \"https://www.linkedin.com/jobs/search/?keywords=Data%20Analyst&location=San%20Francisco%20Bay%20Area&locationId=&geoId=90000084&f_TPR=r86400&position=1&pageNum=0\"\n",
    "scraper_loop(url, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#turn columns into a dataframe\n",
    "curr_date = time.strftime(\"%m-%d-%Y\", time.localtime())\n",
    "date_posted = [curr_date for i in ids]\n",
    "date_scraped = date_posted\n",
    "job_title_category = [\"data analyst\" for i in ids]\n",
    "scraped_dict = {\"date\":date_posted, \"title\": title, \"company\":company, \"location\":location, \"link\":link, \"full description\":descr, \"promoted/easy apply\":promoted_easyapply, \"employment info\":emp_info, \"company info\":co_info, \"recruiter name\":poster_name, \"recruiter profile link\":poster_link, \"connections and alumni\":connections_alumni, \"scraped on\":date_scraped, \"searched job title\":job_title_category}\n",
    "scraped_df = pd.DataFrame(data=scraped_dict, index=ids)\n",
    "scraped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df.drop_duplicates(subset=scraped_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export dataframe to csv\n",
    "name = \"listings_df_\"+\"data_analyst_\"+str(date_scraped[0])+\".csv\"\n",
    "scraped_df.to_csv(\"D:\\\\Scraped Datasets\\\\\"+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Transformation, and Manipulation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "listings_danalyst_09_05_2023 = pd.read_csv(\"D:\\Scraped Datasets\\listings_df_data_analyst_09-05-2023.csv\")\n",
    "listings_dscientist_09_05_2023 = pd.read_csv(\"D:\\Scraped Datasets\\listings_df_data_scientist_09-05-2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_listings = [listings_danalyst_09_05_2023, listings_dscientist_09_05_2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine\n",
    "listings = pd.concat(base_listings, ignore_index=True)\n",
    "listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fix column headers\n",
    "listings.insert(0, \"job id\", listings[\"Unnamed: 0\"])\n",
    "listings.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate location and physical location\n",
    "temp_loc = listings[\"location\"]\n",
    "temp_loc = temp_loc.str.split(\"(\", expand=True)\n",
    "temp_loc[1] = temp_loc[1].str[:-1]\n",
    "listings[\"location\"] = temp_loc[0]\n",
    "listings.insert(5, \"location type\", temp_loc[1])\n",
    "listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create promoted and easy apply columns\n",
    "promote_func = lambda x: True if \"Promoted\" in x else False\n",
    "ea_func = lambda x: True if \"Easy Apply\" in x else False\n",
    "temp_promoted = listings[\"promoted/easy apply\"].apply(promote_func)\n",
    "temp_ea = listings[\"promoted/easy apply\"].apply(ea_func)\n",
    "listings.insert(8, \"promoted\", temp_promoted)\n",
    "listings.insert(9, \"easy apply\", temp_ea)\n",
    "listings.drop([\"promoted/easy apply\"], axis=1, inplace=True)\n",
    "listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings[\"employment info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pay, hours, and experience columns\n",
    "#listings[\"employment info\"][0]\n",
    "#temp_emp_info = listings[\"employment info\"].str.rsplit(\"·\")\n",
    "\n",
    "pay_lower = listings[\"employment info\"].str.extract(r'(\\$[0-9]{1,3},[0-9]{3})')[0]\n",
    "pay_higher = listings[\"employment info\"].str.extract(r'(-\\s\\$[0-9]{1,3},[0-9]{3})')[0].str[1:]\n",
    "pay_type = \n",
    "#temp_hours =\n",
    "#temp_exp =  \n",
    "pay_higher"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
